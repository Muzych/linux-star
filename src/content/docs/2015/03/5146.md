---
author: Arun Pyasi
category: 桌面应用
comments_data:
- date: '2015-03-30 07:33:01'
  message: 转发微博.
  postip: 58.22.157.143
  username: 微博评论
- date: '2015-03-30 07:33:01'
  message: M
  postip: 58.22.157.143
  username: 微博评论
- date: '2015-03-30 07:33:01'
  message: '@我的印象笔记'
  postip: 58.22.157.143
  username: 微博评论
- date: '2015-03-30 07:33:01'
  message: '#华华早安时间# 早安！[微风]'
  postip: 58.22.157.143
  username: 微博评论
- date: '2015-03-30 07:34:22'
  message: 听着&quot;家目录&quot;感觉很奇怪......
  postip: 175.12.146.195
  username: Esteem
- date: '2015-03-30 08:03:01'
  message: 前天我QQ刚发说说，误删了桌面，在桌面下rm -f * 你现在才给我这条
  postip: 223.104.10.178
  username: 微博评论
- date: '2015-03-30 08:03:01'
  message: 黄片太多，删一些重复的。。。。
  postip: 223.104.10.178
  username: 微博评论
- date: '2015-03-30 08:33:01'
  message: '666'
  postip: 220.181.108.92
  username: 微博评论
- date: '2015-03-30 08:33:01'
  message: 这是什么桌面主题
  postip: 220.181.108.92
  username: 微博评论
- date: '2015-03-30 08:33:01'
  message: 回头试试！！
  postip: 220.181.108.92
  username: 微博评论
- date: '2015-03-30 08:39:25'
  message: 文中图片若不是路径，我还以为是这个软件运行在 Windows 8。
  postip: 118.122.120.235
  username: 来自 - 四川成都 的 Chrome/Linux 用户
- date: '2015-03-30 09:03:01'
  message: Linux找回重复文件
  postip: 59.108.68.72
  username: 微博评论
- date: '2015-03-30 09:03:01'
  message: 马
  postip: 59.108.68.72
  username: 微博评论
- date: '2015-03-30 09:22:13'
  message: 哈哈，还真是，这主题用的~
  postip: 106.120.101.58
  username: linux
- date: '2015-03-30 09:23:14'
  message: 嗯，其实我一直习惯“主目录”的说法，可能每个人刚刚开始学习时候，用的什么名称就习惯什么吧。其实，我们很多名词应该逐步标准化了。
  postip: 106.120.101.58
  username: linux
- date: '2015-03-30 09:33:01'
  message: '[挖鼻屎]怎么老有重复的帖子呢'
  postip: 157.55.39.9
  username: 微博评论
- date: '2015-03-30 09:33:01'
  message: '@我的印象笔记'
  postip: 157.55.39.9
  username: 微博评论
- date: '2015-03-30 09:33:01'
  message: '嗯，这个确实合用，片都很占地方~[偷笑]//@白天105键晚上88键: 黄片太多，删一些重复的。。。。'
  postip: 157.55.39.9
  username: 微博评论
- date: '2015-03-30 09:33:01'
  message: '@mark'
  postip: 157.55.39.9
  username: 微博评论
- date: '2015-03-30 09:33:01'
  message: 回复@宋万伟_ops:具体来说，本站发过三个这种软件：attic，dupeGuru 和 FSlint——我也没想到这么多 //@宋万伟_ops:回复@Linux中国:[笑cry]我看标题一样，就没看内容[汗]虽然配图有区别。
    //@宋万伟_ops:[挖鼻屎]怎么老有重复的帖子呢
  postip: 157.55.39.9
  username: 微博评论
- date: '2015-03-30 09:33:01'
  message: '[赞] //@Linux中国:嗯，这个确实合用，片都很占地方~[偷笑]//@白天105键晚上88键: 黄片太多，删一些重复的。。。。'
  postip: 157.55.39.9
  username: 微博评论
- date: '2015-03-30 10:47:34'
  message: 很实用的工具！我喜欢
  postip: 60.247.119.214
  username: kashu
- date: '2015-03-30 11:03:02'
  message: '@我的印象笔记'
  postip: 119.176.58.85
  username: 微博评论
- date: '2015-03-30 18:02:24'
  message: Linux下面这个叫家目录多合适吧
  postip: 221.6.159.249
  username: 朩ダo○
- date: '2015-03-30 22:25:45'
  message: 我刚刚开始学习时，那时候还是 UNIX，就叫主目录:&lt;
  postip: 123.120.85.164
  username: linux
- date: '2015-03-31 08:04:08'
  message: 刚开始还以为基于名字查找的，如果是基于文件名字进行对比，谁敢用啊，嘿嘿，仔细的看了 测试了并不是这样，是基于md5值进行对比的，不错的工具，。
  postip: 116.255.132.12
  username: netb2c
count:
  commentnum: 26
  favtimes: 6
  likes: 0
  sharetimes: 32
  viewnum: 10163
date: '2015-03-30 07:10:00'
editorchoice: false
excerpt: 大家好，今天我们会学习如何在Linux PC或者服务器上找出和删除重复文件。这里有一款工具你可以工具自己的需要使用。 无论你是否正在使用Linux桌面或者服务器，有一些很好的工具能够帮你扫描系统中的重复文件并删除它们来释放空间。图形界面和命令行界面的都有。重复文件是磁盘空间不必要的浪费。毕竟，如果你的确需要在不同的位置享有同一个文件，你可以使用软链接或者硬链接，这样就可以在磁盘的一个地方存储数据了。
  FSlint FSlint 在不同的Linux发行版二进制仓库中都有，包括Ubuntu、Debian、Fedora和Red Hat。只需你运行你的包管理器
fromurl: http://linoxide.com/file-system/find-remove-duplicate-files-linux/
id: 5146
islctt: true
largepic: /data/attachment/album/201503/29/201315q2phszph3ip2spsz.png
permalink: /article-5146-1.html
pic: /data/attachment/album/201503/29/201315q2phszph3ip2spsz.png.thumb.jpg
related:
- displayorder: 0
  raid: 4552
- displayorder: 1
  raid: 4920
- displayorder: 2
  raid: 4992
reviewer: ''
selector: ''
summary: 大家好，今天我们会学习如何在Linux PC或者服务器上找出和删除重复文件。这里有一款工具你可以工具自己的需要使用。 无论你是否正在使用Linux桌面或者服务器，有一些很好的工具能够帮你扫描系统中的重复文件并删除它们来释放空间。图形界面和命令行界面的都有。重复文件是磁盘空间不必要的浪费。毕竟，如果你的确需要在不同的位置享有同一个文件，你可以使用软链接或者硬链接，这样就可以在磁盘的一个地方存储数据了。
  FSlint FSlint 在不同的Linux发行版二进制仓库中都有，包括Ubuntu、Debian、Fedora和Red Hat。只需你运行你的包管理器
tags:
- 重复文件
- FSlint
thumb: false
title: 如何在Linux上找出并删除重复的文件：FSlint
titlepic: false
translator: geekpi
updated: '2015-03-30 07:10:00'
---

大家好，今天我们会学习如何在Linux PC或者服务器上找出和删除重复文件。这里有一款工具你可以工具自己的需要使用。


无论你是否正在使用Linux桌面或者服务器，有一些很好的工具能够帮你扫描系统中的重复文件并删除它们来释放空间。图形界面和命令行界面的都有。重复文件是磁盘空间不必要的浪费。毕竟，如果你的确需要在不同的位置享有同一个文件，你可以使用软链接或者硬链接，这样就可以在磁盘的一个地方存储数据了。


### FSlint


[FSlint](http://www.pixelbeat.org/fslint/) 在不同的Linux发行版二进制仓库中都有，包括Ubuntu、Debian、Fedora和Red Hat。只需你运行你的包管理器并安装“fslint”包就行。这个工具默认提供了一个简单的图形化界面，同样也有包含各种功能的命令行版本。


不要担心FSlint的图形化界面太复杂。默认情况下，它会自动选中Duplicate窗格，并以你的家目录作为搜索路径。


要安装fslint，若像我这样运行的是Ubuntu，这里是默认的命令：



```
$ sudo apt-get install fslint

```

这里还有针对其他发行版的安装命令：


Debian：



```
svn checkout http://fslint.googlecode.com/svn/trunk/ fslint-2.45
cd fslint-2.45
dpkg-buildpackage -I.svn -rfakeroot -tc
sudo dpkg -i ../fslint_2.45-1_all.deb

```

Fedora：



```
sudo yum install fslint

```

OpenSUSE：



```
[ -f /etc/mandrake-release ] && pkg=rpm
[ -f /etc/SuSE-release ] && pkg=packages
wget http://www.pixelbeat.org/fslint/fslint-2.42.tar.gz
sudo rpmbuild -ta fslint-2.42.tar.gz
sudo rpm -Uvh /usr/src/$pkg/RPMS/noarch/fslint-2.42-1.*.noarch.rpm

```

对于其他发行版：



```
wget http://www.pixelbeat.org/fslint/fslint-2.44.tar.gz
tar -xzf fslint-2.44.tar.gz
cd fslint-2.44
(cd po && make)
./fslint-gui

```

要在Ubuntu中运行fslint的GUI版本fslint-gui, 使用Alt+F2运行命令或者在终端输入：



```
$ fslint-gui

```

默认情况下，它会自动选中Duplicate窗格，并以你的家目录作为搜索路径。你要做的就是点击Find按钮，FSlint会自动在你的家目录下找出重复文件列表。


![Delete Duplicate files with Fslint](/data/attachment/album/201503/29/201315q2phszph3ip2spsz.png)


点击按钮来删除任何你要删除的文件，并且可以双击预览。


完成这一切后，我们就成功地删除你系统中的重复文件了。


**注意** ，命令行工具默认不在环境的路径中，你不能像典型的命令那样运行它。在Ubuntu中，你可以在/usr/share/fslint/fslint下找到它。因此，如果你要在一个单独的目录运行fslint完整扫描，下面是Ubuntu中的运行命令：



```
cd /usr/share/fslint/fslint

./fslint /path/to/directory

```

**这个命令实际上并不会删除任何文件。它只会打印出重复文件的列表-你需要自己做接下来的事。**



```
$ /usr/share/fslint/fslint/findup --help
find dUPlicate files.
Usage: findup [[[-t [-m|-d]] | [--summary]] [-r] [-f] paths(s) ...]

If no path(s) specified then the current directory is assumed.

When -m is specified any found duplicates will be merged (using hardlinks).
When -d is specified any found duplicates will be deleted (leaving just 1).
When -t is specfied, only report what -m or -d would do.
When --summary is specified change output format to include file sizes.
You can also pipe this summary format to /usr/share/fslint/fslint/fstool/dupwaste
to get a total of the wastage due to duplicates.

```

![fslint help](/data/attachment/album/201503/29/201317rhoizhd4zozqam80.png)




---


via: <http://linoxide.com/file-system/find-remove-duplicate-files-linux/>


作者：[Arun Pyasi](http://linoxide.com/author/arunp/) 译者：[geekpi](https://github.com/geekpi) 校对：[wxy](https://github.com/wxy)


本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创翻译，[Linux中国](http://linux.cn/) 荣誉推出